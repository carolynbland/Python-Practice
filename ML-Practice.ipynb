{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db3f843",
   "metadata": {},
   "source": [
    "## Machine Learning Cheat Sheet \n",
    "#### This notebook contains a collection of machine learning techniques and practice. It is a work in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd55ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs, make_moons, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb79b12",
   "metadata": {},
   "source": [
    "### 1. Supervised learning \n",
    "* Data is labeled and model is trained to make correct predictions \n",
    "* Regression: used to predict real numerical values e.g. home sales prices, stock market prices \n",
    "* Classification: classify things into categories e.g. email spam filters, fraud detection, image classification \n",
    "\n",
    "##### K-Nearest Neighbor Classification\n",
    "* Xtrain,Xtest,Ytrain,Ytest = train_test_split(X,Y, train_size=0.7, random_state_42)\n",
    "* model = KNeighborsClassifier(n_neighbors = 3) \n",
    "* model.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "* accuracy_score(ytest, y_model)\n",
    "* for i in range(len(ytest)):\n",
    "    if ytest[i] != y_model[i]:\n",
    "        plt.plot(Xtest[i,0],Xtest[i,1],'sk',markersize=10) %plot the misclassified points\n",
    "* matrix = confusion_matrix(ytest, y_model) %visualize accuracy with confusion matrix\n",
    "\n",
    "##### Random Forest Classifier \n",
    "* forest= RandomForestClassifier(n_estimators=5, random_state=2) %define parameters for model\n",
    "* y_model = forest.fit(Xtrain, ytrain).predict(Xtest)\n",
    "* sns.pairplot(df, hue='Item to color by', height=1.5) %examine the features\n",
    "* sns.regplot(), sns.lmplot() %used to visualize linear relationship\n",
    "* n_features = model_rf.n_features_ %get number of features\n",
    "* plt.barh(np.arange(n_features), model_rf.feature_importances_, align='center') %plot feature importances\n",
    "\n",
    "##### Other Classifiers\n",
    "* logreg = LogisticRegression()\n",
    "* y_pred = logreg.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "\n",
    "* gaussian = GaussianNB()\n",
    "* y_pred = gaussian.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "\n",
    "* svc = SVC()\n",
    "* y_pred = svc.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "\n",
    "* perceptron = Perceptron(class_weight='balanced')\n",
    "* y_pred = perceptron.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "\n",
    "* gbk = GradientBoostingClassifier()\n",
    "* y_pred = gbk.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "\n",
    "* ada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\n",
    "* y_pred = ada.fit(Xtrain,Ytrain).predict(Xtest)\n",
    "\n",
    "\n",
    "### 2. Unsupervised learning\n",
    "* Data is not labeled \n",
    "* Model tries to identify patterns without external help \n",
    "* Clustering: providing purchase recommendations for an ecommerce website \n",
    "* Anomaly Detection: e.g. someone using your credit card \n",
    "\n",
    "##### Visualize dataset using the t-SNE manifold learning algorithm \n",
    "* tsne = TSNE() \n",
    "* data_tsne = tsne.fit_transform(data.data)\n",
    "* df_data = pd.DataFrame(digits_tsne, columns=['TSNE1','TSNE2'])  \n",
    "* df_data[\"value\"] = data.target\n",
    "* sns.lmplot(\"TSNE1\", \"TSNE2\", hue='value', data=df_data, fit_reg=False);\n",
    "\n",
    "##### K-Means Clustering \n",
    "* data_km = KMeans(n_clusters=10, random_state=0)\n",
    "* data_clusters_km = data_km.fit_predict(X_data)\n",
    "* Can use Elbow method to help determine number of clusters - want small value of k that still has a low SSE \n",
    "*   sse=[]\n",
    "\n",
    "    for i in range(1,20):\n",
    "        kmeans=KMeans(n_clusters=i,init='k-means++',)\n",
    "        kmeans.fit(X_digits)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    plt.plot(range(1,20),sse)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('SSE' );\n",
    "\n",
    "\n",
    "### 3. Reinforcement Learning\n",
    "* Conceptually similar to human learning processes \n",
    "* Learns best set of actions to take given a current environment in order to get most reward overtime e.g. recommendations by netflix \n",
    "\n",
    "### 4. Deep Learning \n",
    "* Tries to loosely emulate how the human brain works \n",
    "* Applications: Natural language processing, image audio and video analysis, time series forecasting, etc \n",
    "* Requires typically very large datasets of labeled data and is computationally expensive \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c7f470",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
